---
title: "The Game is Afoot!"
runtime: shiny
output: 
  flexdashboard::flex_dashboard:
    social: menu
    source_code: embed
    orientation: columns
    vertical_layout: fill
---

```{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
library(dplyr)
library(stm)
library(quanteda)
library(tidytext)
library(ggplot2)
library(scales)

load("topic_model.rda")
load("sherlock_dfm.rda")

td_beta <- tidy(topic_model)
td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(sherlock_dfm))

```

Documents by topic {data-icon="fa-bar-chart"}
====================================================================

### Which documents have the highest probability of being generated from each topic? {data-height=800}

```{r}
td_gamma %>% 
    mutate(document = factor(document, levels = rev(unique(document)))) %>%
    group_by(document) %>%
    top_n(1) %>%
    ungroup %>%
    ggplot(aes(document, gamma, label = document, fill = as.factor(topic))) +
    geom_col() +
    geom_text(aes(document, 0.01), hjust = 0,
              color = "white", size = 2.5) +
    scale_fill_manual(values = c("#F48024", "#0077CC", "#5FBA7D", 
                                 "#8C60A7", "#34495E", "#CDDC39")) +
    scale_y_continuous(expand = c(0,0),
                       labels = percent_format()) +
    coord_flip() +
    theme_minimal() +
    theme(axis.text.y=element_blank()) +
    labs(x = NULL, y = expression(gamma), fill = "Topic")    

```

### Topic modeling {data-height=200}

Topic modeling is a method for unsupervised classification of text documents 
which finds natural groups of items even when we’re not sure what we’re looking for.
This app explores the results of a [Structural Topic Model](http://www.structuraltopicmodel.com/)
for the twelve short stories in 
[*The Adventures of Sherlock Holmes*](https://www.gutenberg.org/ebooks/1661).
Which stories are more similar? Which stories are more about women, or more about events at night, 
or more about... geese?! Use each tab to find out.

Explore our book [*Text Mining with R: A Tidy Approach*](http://tidytextmining.com/) 
to learn more about how to implement topic modeling using tidy data principles.


Words by topic {data-icon="fa-list"}
====================================================================

### Which words have the highest probability of being generated from each topic?

```{r}
fillCol(height = 600, flex = c(NA, 1), 
  inputPanel(
    selectizeInput("topic2", "Choose a topic", choices = unique(td_beta$topic))
  ),
  plotOutput("topicPlot", height = "100%")
)

output$topicPlot <- renderPlot({
    td_beta %>%
        filter(topic == as.integer(input$topic2)) %>%
        top_n(15, beta) %>%
        mutate(term = reorder(term, beta)) %>%
        ggplot(aes(term, beta, fill = beta)) +
        geom_col(show.legend = FALSE) +
        scale_fill_gradient(low = "#5FBA7D", high = "#108AEC") +
        scale_y_continuous(expand = c(0,0),
                           labels = percent_format()) +
        coord_flip() +
        theme_minimal(base_size = 18) +
        labs(x = NULL, y = expression(beta))
})

```



Explore words {data-icon="fa-search"}
====================================================================

### Which topic is a word most likely to be generated from?

```{r}
fillCol(height = 600, flex = c(NA, 1), 
  inputPanel(
    selectizeInput("word", "Choose a word", choices = unique(td_beta$term), selected = "watson")
  ),
  plotOutput("wordPlot", height = "100%")
)

output$wordPlot <- renderPlot({
    td_beta %>% 
        filter(term == input$word) %>% 
        arrange(-beta) %>% 
        top_n(10) %>% 
        mutate(topic = reorder(topic, beta)) %>%
        ggplot(aes(topic, beta, fill = beta)) +
        geom_col(show.legend = FALSE) +
        scale_fill_gradient(low = "#5FBA7D", high = "#108AEC") +
        scale_y_continuous(expand = c(0,0),
                           labels = percent_format()) +
        coord_flip() +
        theme_minimal(base_size = 18) +
        labs(x = NULL, y = expression(beta))
})

```



